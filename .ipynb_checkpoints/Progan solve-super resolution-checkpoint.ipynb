{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import *\n",
    "from model import *\n",
    "from utils import *\n",
    "from train import *\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from math import log2\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as nnf\n",
    "torch.manual_seed(2)\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from skimage.measure import compare_psnr\n",
    "from skimage.transform import rescale, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(fname):\n",
    "    \n",
    "    downsample = nnf.interpolate(fname, size=None, scale_factor=0.25, mode='bicubic')  # use either of size or scale factor\n",
    "    \n",
    "    return downsample\n",
    "\n",
    "def ProGAN_solver(test_img='Gabrielle Anwar',savedir='out_images_test'):\n",
    "    savedir = 'downsample'\n",
    "    test_img = 'gabrielle_test_cubic'\n",
    "    \n",
    "    nIter = 10001\n",
    "\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    \n",
    "    workers = 2\n",
    "    ngpu = 1\n",
    "    batch_size = 1  # just to make 4 dim\n",
    "    iters = np.array(np.geomspace(10,10,nIter),dtype=int)\n",
    "    fname = '../ProGAN_g/downsample/{}.jpg'.format(test_img)\n",
    "    image = io.imread(fname)\n",
    "    #x_test = resize(image, (int(config.Z_DIM*2), int(config.Z_DIM*2)),anti_aliasing=True,preserve_range=True,mode='reflect')\n",
    "    image_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                        transforms.ToTensor()])\n",
    "    test_images = image_transform(image)\n",
    "    test_images = test_images.unsqueeze(0)\n",
    "    print(test_images.shape)\n",
    "    #test_images = torch.Tensor(np.transpose(image[:batch_size,:,:,:],[0,3,1,2]))\n",
    "    Z_DIM = 256 #latent dimensionality of GAN (fixed)\n",
    "    IN_CHANNELS = 256\n",
    "    CHANNELS_IMG = 3\n",
    "    io.imsave('{}/gt.png'.format(savedir),(image).astype(np.uint8))\n",
    "    \n",
    "    genPATH = './ProGAN_g/generator.pth'\n",
    "    #gen = Generator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(device)\n",
    "    # LOAD THE WEIGHTS AT THIS STAGE\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        \n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        gen = nn.DataParallel(gen, list(range(ngpu)))\n",
    "        print('lol')\n",
    "    if os.path.isfile(genPATH):\n",
    "        if device.type == 'cuda':\n",
    "            gen.load_state_dict(torch.load(genPATH))\n",
    "        elif device.type=='cpu':\n",
    "            gen.load_state_dict(torch.load(genPATH,map_location=torch.device('cpu')))\n",
    "        else:\n",
    "            raise Exception(\"Unable to load model to specified device\")\n",
    "\n",
    "        print(\"************ Generator weights restored! **************\")\n",
    "        gen.eval()\n",
    "    gen.eval()\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.SmoothL1Loss(reduction='sum')\n",
    "    FIXED_NOISE = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)\n",
    "    #z_prior = torch.zeros(165,Z_DIM,1,1,requires_grad=True,device=device)\n",
    "    optimizerZ = optim.RMSprop([FIXED_NOISE], lr=5e-3)\n",
    "\n",
    "    real_cpu = test_images.to(device)\n",
    "    \n",
    "    for iters in range(nIter):\n",
    "        optimizerZ.zero_grad()\n",
    "        opt_gen.zero_grad()\n",
    "        z2 = torch.clamp(FIXED_NOISE,-1.,1.)\n",
    "        #z2 = FIXED_NOISE\n",
    "        fake = 0.5*gen(z2, 1e-5,int(log2(config.START_TRAIN_AT_IMG_SIZE / 4)))+0.5\n",
    "        #print('gen shape:',fake.shape)\n",
    "        fake = downsampling(fake)\n",
    "        #print('downsam_gen shape:',fake.shape)\n",
    "        cost = 0\n",
    "        for i in range(3):\n",
    "            y_gt = real_cpu[:,i,:,:]\n",
    "            y_est = fake[:,i,:,:]\n",
    "            cost += criterion(y_gt,y_est)\n",
    "        \n",
    "        cost.backward()\n",
    "        optimizerZ.step()\n",
    "        opt_gen.step()\n",
    "        if (iters % 100 == 0):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z2 = torch.clamp(FIXED_NOISE,-1.,1.)\n",
    "                fake = 0.5*gen(z2, 1e-5,int(log2(config.START_TRAIN_AT_IMG_SIZE / 4))).detach().cpu() + 0.5\n",
    "                G_imgs = np.transpose(fake.detach().cpu().numpy(),[0,1,2,3])\n",
    "                G_imgs = torch.Tensor(G_imgs)\n",
    "            \n",
    "            \n",
    "            #psnr = compare_psnr(x_test_,G_imgs,data_range=1.0)\n",
    "            print('Iter: {:d}, Error: {:.3f}'.format(iters,cost.item()))\n",
    "            #io.imsave('{}/inv_solution_iters_{}.png'.format(savedir,str(iters).zfill(4)),(G_imgs).astype(np.uint8))\n",
    "            #save_image(G_imgs, f'out_images_test/inv_solution_iters_{iters}.png')\n",
    "            save_image(G_imgs,'{}/inv_solution_iters_{}.png'.format(savedir, iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "=> Loading checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "C:\\Users\\gaura\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Error: 3352.699\n",
      "Iter: 100, Error: 66.079\n",
      "Iter: 200, Error: 35.753\n",
      "Iter: 300, Error: 22.555\n",
      "Iter: 400, Error: 16.481\n",
      "Iter: 500, Error: 16.687\n",
      "Iter: 600, Error: 14.407\n",
      "Iter: 700, Error: 15.964\n",
      "Iter: 800, Error: 12.611\n",
      "Iter: 900, Error: 18.072\n",
      "Iter: 1000, Error: 13.999\n",
      "Iter: 1100, Error: 7.056\n",
      "Iter: 1200, Error: 6.152\n",
      "Iter: 1300, Error: 7.432\n",
      "Iter: 1400, Error: 4.901\n",
      "Iter: 1500, Error: 18.092\n",
      "Iter: 1600, Error: 6.534\n",
      "Iter: 1700, Error: 7.510\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'downsample/inv_solution_iters_1700.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a67c599b299d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mProGAN_solver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-1b4382b0335b>\u001b[0m in \u001b[0;36mProGAN_solver\u001b[1;34m(test_img, savedir)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m#io.imsave('{}/inv_solution_iters_{}.png'.format(savedir,str(iters).zfill(4)),(G_imgs).astype(np.uint8))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m#save_image(G_imgs, f'out_images_test/inv_solution_iters_{iters}.png')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0msave_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_imgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'{}/inv_solution_iters_{}.png'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\utils.py\u001b[0m in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, nrow, padding, normalize, range, scale_each, pad_value, format)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2097\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2099\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'downsample/inv_solution_iters_1700.png'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ProGAN_solver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
